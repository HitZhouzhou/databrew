bash: kill: nohup: arguments must be process or job IDs
bash: kill: ./auto_runall.sh: arguments must be process or job IDs
nohup: ignoring input
/data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_fwd")
[1;36m(VllmWorkerProcess pid=1103298)[0;0m /data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[1;36m(VllmWorkerProcess pid=1103298)[0;0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[1;36m(VllmWorkerProcess pid=1103297)[0;0m /data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[1;36m(VllmWorkerProcess pid=1103297)[0;0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[1;36m(VllmWorkerProcess pid=1103302)[0;0m /data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[1;36m(VllmWorkerProcess pid=1103302)[0;0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[1;36m(VllmWorkerProcess pid=1103301)[0;0m /data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[1;36m(VllmWorkerProcess pid=1103301)[0;0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[1;36m(VllmWorkerProcess pid=1103300)[0;0m /data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[1;36m(VllmWorkerProcess pid=1103300)[0;0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[1;36m(VllmWorkerProcess pid=1103299)[0;0m /data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[1;36m(VllmWorkerProcess pid=1103299)[0;0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[1;36m(VllmWorkerProcess pid=1103296)[0;0m /data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[1;36m(VllmWorkerProcess pid=1103296)[0;0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
/data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("xformers_flash::flash_bwd")
[1;36m(VllmWorkerProcess pid=1103298)[0;0m /data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[1;36m(VllmWorkerProcess pid=1103297)[0;0m /data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[1;36m(VllmWorkerProcess pid=1103298)[0;0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[1;36m(VllmWorkerProcess pid=1103297)[0;0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[1;36m(VllmWorkerProcess pid=1103302)[0;0m /data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[1;36m(VllmWorkerProcess pid=1103302)[0;0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[1;36m(VllmWorkerProcess pid=1103301)[0;0m /data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[1;36m(VllmWorkerProcess pid=1103301)[0;0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[1;36m(VllmWorkerProcess pid=1103300)[0;0m /data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[1;36m(VllmWorkerProcess pid=1103300)[0;0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[1;36m(VllmWorkerProcess pid=1103299)[0;0m /data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[1;36m(VllmWorkerProcess pid=1103299)[0;0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[1;36m(VllmWorkerProcess pid=1103296)[0;0m /data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[1;36m(VllmWorkerProcess pid=1103296)[0;0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:00<00:01,  3.32it/s]
Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:00<00:01,  3.44it/s]
Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:00<00:00,  3.44it/s]
Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:01<00:00,  3.44it/s]
Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:01<00:00,  3.40it/s]
Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:01<00:00,  3.55it/s]
Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:01<00:00,  3.48it/s]

[rank0]: Traceback (most recent call last):
[rank0]:   File "/data/zjy/databrew/code/data_generation/bonito_sample.py", line 23, in <module>
[rank0]:     bonito = Bonito("../../models/Bonito-test-v0.1", max_model_len=30000, tensor_parallel_size=8)
[rank0]:   File "/data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/vllm/entrypoints/llm.py", line 178, in __init__
[rank0]:     self.llm_engine = LLMEngine.from_engine_args(
[rank0]:   File "/data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/vllm/engine/llm_engine.py", line 550, in from_engine_args
[rank0]:     engine = cls(
[rank0]:   File "/data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/vllm/engine/llm_engine.py", line 331, in __init__
[rank0]:     self._initialize_kv_caches()
[rank0]:   File "/data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/vllm/engine/llm_engine.py", line 473, in _initialize_kv_caches
[rank0]:     self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)
[rank0]:   File "/data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/vllm/executor/distributed_gpu_executor.py", line 63, in initialize_cache
[rank0]:     self._run_workers("initialize_cache",
[rank0]:   File "/data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/vllm/executor/multiproc_gpu_executor.py", line 199, in _run_workers
[rank0]:     driver_worker_output = driver_worker_method(*args, **kwargs)
[rank0]:   File "/data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/vllm/worker/worker.py", line 265, in initialize_cache
[rank0]:     self._init_cache_engine()
[rank0]:   File "/data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/vllm/worker/worker.py", line 270, in _init_cache_engine
[rank0]:     self.cache_engine = [
[rank0]:   File "/data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/vllm/worker/worker.py", line 271, in <listcomp>
[rank0]:     CacheEngine(self.cache_config, self.model_config,
[rank0]:   File "/data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/vllm/worker/cache_engine.py", line 66, in __init__
[rank0]:     self.gpu_cache = self._allocate_kv_cache(
[rank0]:   File "/data/zjy/anaconda3/envs/bonito/lib/python3.9/site-packages/vllm/worker/cache_engine.py", line 85, in _allocate_kv_cache
[rank0]:     torch.zeros(kv_cache_shape,
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 178.12 MiB is free. Process 1102421 has 18.30 GiB memory in use. Including non-PyTorch memory, this process has 5.15 GiB memory in use. Of the allocated memory 4.51 GiB is allocated by PyTorch, and 13.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/data/zjy/anaconda3/envs/bonito/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
